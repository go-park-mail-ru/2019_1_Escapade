version: '3.7'


volumes:
  prometheus: {}
  grafana: {}

# networks

networks:
  traefik:
    driver: overlay
    external: true
  net:
    driver: overlay
    attachable: true

# secrets

secrets:
  db_create_user:
    file: ./../../../../extra/postgresql/main/init/1_secret.sql
  db_create_tables:
    file: ./../../../../extra/postgresql/main/init/2_tables.sql

# configs

configs:
  dockerd_config:
    file: ./../../../../extra/dockerd-exporter/Caddyfile
  prometheus_config:
    file: ./../../../../extra/prometheus/conf/prometheus.yml
  node_rules:
    file: ./../../../../extra/prometheus/rules/swarm_node.rules.yml
  task_rules:
    file: ./../../../../extra/prometheus/rules/swarm_task.rules.yml


x-network: &network
  networks:
    - traefik

x-logging: &logging
  logging:
    options:
      max-size: 10m

x-consul: &consul
  image: consul:1.0.2
  networks:
    backend-overlay:
      aliases:
        - consul.cluster
  environment:
    - IP=$IP
  volumes:
    - ~/2019_1_Escapade/extra/consul/config:/config

x-backend: &backend-server
  <<: *logging
  <<: *network
  volumes:
    - ./../../../..:/2019_1_Escapade
  labels:
    - "org.label-schema.group=monitoring"

x-manager: &manager
  restart_policy:
    condition: on-failure
  placement:
    constraints: [node.role == manager] 

x-worker: &worker
  placement:
    constraints: [node.role == worker] 

services:

  ### Load balancer ###

  traefik:
    <<: *logging
    <<: *network
    image: traefik:v2.1
    ports:
      - 80:80     # The HTTP port
      - 443:443     # The HTTPS port
      - 8080:8080 # The Web UI
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events
      - ./../../../../extra/traefik/traefik.toml:/traefik.toml # Traefik configuration file
    deploy: 
      <<: *manager
    
#   ### Database servers ###

  pg:
    <<: *logging
    <<: *network
    image: postgres:11.1
    secrets:
      - source: db_create_user
        target: "/docker-entrypoint-initdb.d/1.sql"
        mode: 0755  #Grant read and execute permission to owner, execute and read to group and others
        uid: "0"  #The uid for root is due to the permissions for the init.db folder for image
      - source: db_create_tables
        target: "/docker-entrypoint-initdb.d/2.sql"
        mode: 0755  #Grant read and execute permission to owner, execute and read to group and others
        uid: "0"  #The uid for root is due to the permissions for the init.db folder for image
    deploy: 
      <<: *manager

  # pg-auth:
  #   <<: *logging
  #   image: postgres:11.2-alpine
  #   networks:
  #     - backend-overlay
  #   environment:
  #     POSTGRES_USER: 'auth' 
  #     POSTGRES_PASSWORD: 'auth'
  #     POSTGRES_DB: 'authbase'
  #   ports:
  #     - "5431:5432" 
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.role == manager]    

  # pg-ery:
  #   <<: *logging
  #   image: smartphonejava/pg-ery:latest
  #   ports:
  #     - "5430:5432"   
  #   networks:
  #     - backend-overlay
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.role == manager] 

  ### Backend servers ###

  api:
    <<: *backend-server
    image: smartphonejava/api:latest
    ports:
      - 3001:3001
    environment:
      - AUTH_ADDRESS=http://auth:3003 
      - DB_CONN_STRING=postgres://rolepade:escapade@pg:5432/escabase?sslmode=disable
      - network=traefik
      - entrypoint=web
    command: 
      - --config=2019_1_Escapade/cmd/services/api/api.json
      - --photo=2019_1_Escapade/internal/photo/photo.json
      - --secrets=2019_1_Escapade/secret.json 
      - --port=3001
      - --name=api
      - --subnet=10.10.8.
      - --consul=consul:8500
    deploy:
      <<: *worker
      replicas: 4
      update_config:
        parallelism: 3
      restart_policy:
        condition: on-failure
        window: 5s

  # chat:
  #   <<: *backend-server
  #   image: smartphonejava/chat:latest
  #   environment:
  #     - DB_CONN_STRING=postgres://rolepade:escapade@pg:5432/escabase?sslmode=disable
  #     - CONSUL_ADDRESS=consul-agent
  #   ports:
  #     - 3066-3087:3060
  #   command: 
  #     - 2019_1_Escapade/cmd/services/chat/chat.json
  #     - "3060"
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.role == manager] 

  # auth:
  #   <<: *backend-server
  #   image: smartphonejava/auth:latest
  #   ports:
  #     - 3022-3043:3003
  #   environment:
  #     - DB_CONN_STRING=postgres://rolepade:escapade@pg:5432/escabase?sslmode=disable
  #     - CONSUL_ADDRESS=consul-agent
  #   command: 
  #     - 2019_1_Escapade/cmd/services/auth/auth.json
  #     - "3003"
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.role == manager] 

  # game:
  #   <<: *backend-server
  #   image: smartphonejava/game:latest
  #   ports:
  #     - 3044-3055:3002
  #   environment:
  #     - DB_CONN_STRING=postgres://rolepade:escapade@pg:5432/escabase?sslmode=disable
  #     - CONSUL_ADDRESS=consul-agent
  #     - AUTHSERVICE_URL=auth:3333
  #     - PORT_GAME_URL=:3002
  #   command: 
  #     - 2019_1_Escapade/cmd/services/game/game.json
  #     - 2019_1_Escapade/cmd/services/game/photo.json
  #     - 2019_1_Escapade/secret.json 
  #     - 2019_1_Escapade/cmd/services/game/field.json
  #     - 2019_1_Escapade/cmd/services/game/room.json
  #     - "3002"
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.role == manager] 

#   ### Monitoring services ###

  prometheus:
    <<: *logging
    <<: *network
    image: prom/prometheus:v2.5.0
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention=${PROMETHEUS_RETENTION:-24h}'
    volumes:
      - prometheus:/prometheus
    configs:
      - source: prometheus_config
        target: /etc/prometheus/prometheus.yml
      - source: node_rules
        target: /etc/prometheus/swarm_node.rules.yml
      - source: task_rules
        target: /etc/prometheus/swarm_task.rules.yml
    ports:
      - "9090:9090"
    deploy:
      <<: *manager
      labels:
        - traefik.enable=true
        - traefik.http.services.prometheus.loadbalancer.server.port=9090
        - traefik.http.routers.prometheus.rule=PathPrefix(`/prometheus`, `/graph`)
       
        - traefik.http.middlewares.pr-chain.chain.middlewares=pr-strip,pr-add
        - traefik.http.middlewares.pr-strip.stripprefix.prefixes=/prometheus
        - traefik.http.middlewares.pr-add.addprefix.prefix=/prometheus

        # - "traefik.http.middlewares.example.stripprefix.prefixes=/prometheus"
        # - "traefik.http.middlewares.example.stripprefix.forceslash=false"
        # - traefik.http.routers.prometheus.middlewares=example
        - traefik.http.routers.prometheus.middlewares=pr-strip
        - traefik.http.routers.prometheus.service=prometheus
        - traefik.http.routers.prometheus.entrypoints=web
        - traefik.docker.network=traefik
      resources:
        limits:
          memory: 2048M
        reservations:
          memory: 128M

  dockerd-exporter:
    <<: *logging
    <<: *network
    image: stefanprodan/caddy
    environment:
      - DOCKER_GWBRIDGE_IP=172.19.0.1
    configs:
      - source: dockerd_config
        target: /etc/caddy/Caddyfile
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  cadvisor:
    <<: *logging
    <<: *network
    image: google/cadvisor
    command: 
      - -logtostderr 
      - -docker_only
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  node-exporter:
    <<: *logging
    <<: *network
    image: stefanprodan/swarmprom-node-exporter:v0.16.0
    environment:
      - NODE_ID={{.Node.ID}}
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.textfile.directory=/etc/node-exporter/'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.ipvs'
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  grafana:
    <<: *logging
    <<: *network
    image: stefanprodan/swarmprom-grafana:5.3.4
    environment:
     - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
     - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
     - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - 3000:3000
    volumes:
      - grafana:/var/lib/grafana
    deploy:
      <<: *manager
      labels:
        - traefik.enable=true
        - traefik.http.services.grafana.loadbalancer.server.port=3000
        - traefik.http.routers.grafana.rule=PathPrefix(`/grafana`)
        - traefik.http.routers.grafana.service=grafana
        - traefik.http.routers.grafana.entrypoints=web
        - traefik.docker.network=traefik
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  ### Service discovery ###

  consul: 
    <<: *logging
    networks:
      traefik:
        aliases:
          - consul.cluster
    image: consul:1.6.2
    ports:
      - "8500:8500"
      - "8300:8300"
      - "8400:8400"
      - "8301:8301/tcp"
      - "8302:8302/tcp"
      - "8301:8301/udp"
      - "8302:8302/udp"
      - "8610:53/udp"
      #-node bootstrap
    command: 
      - agent 
      - -config-file=./config/bootstrap.json
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./../../../../extra/consul/config:/config
    deploy:
      <<: *manager

#   ### Frontend servers ###

  # front:
  #   <<: *logging
  #   image: smartphonejava/front:latest
  #   networks:
  #     - backend-overlay
  #   ports:
  #     - "8088:8080"   
  #   command: start
  #   labels:
  #     - traefik.enable=true
  #     - "traefik.frontend.rule=PathPrefixStrip: /somepath"
  #     - traefik.port=8088
  #     - traefil.docker.network=backend-overlay
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.role != manager] 

# # volumes

# x-consul: &consul
#   <<: *logging  
#   image: consul:1.6.2
#   <<: *network
#   volumes:
#     - ./../../../../extra/consul/config:/config

# consul-server:
#   <<: *logging
#   networks:
#     traefik:
#       aliases:
#         - consul.cluster
#   image: consul:1.6.2
#   ports:
#     - "8511:8500"
#     - "8311:8301/tcp"
#     - "8312:8302/tcp"
#     - "8311:8301/udp"
#     - "8312:8302/udp"
#   command: agent -join consul -config-file=./config/server.json
#   volumes:
#     - ./../../../../extra/consul/config:/config
#   deploy:
#     replicas: 3
#     restart_policy:
#       condition: on-failure
#     placement:
#       constraints: [node.role == manager] 

# consul-agent:
#   <<: *logging
#   networks:
#     traefik:
#       aliases:
#         - consul.cluster
#   image: consul:1.6.2
#   ports:
#     - "8501:8500"
#     - "8321:8301/tcp"
#     - "8322:8302/tcp"
#     - "8321:8301/udp"
#     - "8322:8302/udp"
#   command: agent -join consul-server -config-file=./config/client.json 
#   environment:
#     - CONSUL_BIND_INTERFACE=eth0
#   volumes:
#     - ./../../../../extra/consul/config:/config
#   labels:
#     - "traefik.frontend.rule=Host:consul-agent.2019-1-escapade.docker.localhost"
#   deploy:
#     replicas: 3
#     restart_policy:
#       condition: on-failure
#     placement:
#       constraints: [node.role == manager] 
